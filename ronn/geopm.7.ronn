geopm(7) -- global extensible open power manager
=======================================================

[//]: # (Copyright (c) 2015, 2016, 2017, 2018, Intel Corporation)
[//]: # ()
[//]: # (Redistribution and use in source and binary forms, with or without)
[//]: # (modification, are permitted provided that the following conditions)
[//]: # (are met:)
[//]: # ()
[//]: # (    * Redistributions of source code must retain the above copyright)
[//]: # (      notice, this list of conditions and the following disclaimer.)
[//]: # ()
[//]: # (    * Redistributions in binary form must reproduce the above copyright)
[//]: # (      notice, this list of conditions and the following disclaimer in)
[//]: # (      the documentation and/or other materials provided with the)
[//]: # (      distribution.)
[//]: # ()
[//]: # (    * Neither the name of Intel Corporation nor the names of its)
[//]: # (      contributors may be used to endorse or promote products derived)
[//]: # (      from this software without specific prior written permission.)
[//]: # ()
[//]: # (THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS)
[//]: # ("AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT)
[//]: # (LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR)
[//]: # (A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT)
[//]: # (OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,)
[//]: # (SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT)
[//]: # (LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,)
[//]: # (DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY)
[//]: # (THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT)
[//]: # ((INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY LOG OF THE USE)
[//]: # (OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.)

## DESCRIPTION
Global Extensible Open Power Manager (GEOPM) is an extensible
power management framework targeting high performance computing.  The
library can be extended to support new control algorithms and new
hardware power management features.  The GEOPM package provides built
in features ranging from static management of power policy for each
individual compute node, to dynamic coordination of power policy and
performance across all of the compute nodes hosting one MPI job on a
portion of a distributed computing system.  The dynamic coordination
is implemented as a hierarchical control system for scalable
communication and decentralized control.  The hierarchical control
system can optimize for various objective functions including
maximizing global application performance within a power bound.  The
root of the control hierarchy tree can communicate through shared
memory with the system resource management daemon to extend the
hierarchy above the individual MPI job level and enable management of
system power resources for multiple MPI jobs and multiple users by the
system resource manager.  The GEOPM package provides the libgeopm
library, the libgeopmpolicy library, the geopmctl application and the
geopmpolicy application.  The libgeopm library can be called within
MPI applications to enable application feedback for informing the
control decisions.  If modification of the target application is not
desired then the geopmctl application can be run concurrently with the
target application.  In this case, target application feedback is
inferred by querying the hardware through Model Specific Registers
(MSRs).  With either method (libgeopm or geopmctl), the control
hierarchy tree writes processor power policy through MSRs to enact
policy decisions.  The libgeopmpolicy library is used by a resource
manager to set energy policy control parameters for MPI jobs.  Some
features of libgeopmpolicy are available through the geopmpolicy
application including support for static control.


## JOB LAUNCH

**geopmsrun(1)**: SLURM application launch wrapper

**geopmaprun(1)**: ALPS application launch wrapper

## APPLICATION PROFILING

**geopm_prof_c(3)**: Application profiling interfaces

**geopm_fortran(3)**: GEOPM Fortran interfaces

## ANALYSIS TOOLS

**geopmanalysis(1)**: Run application and visualize results

**geopmplotter(1)**: Visualize reports and traces

**geopmread(1)**: Query platform information

**geopmwrite(1)**: Modify platform state

**geopmbench(1)**: Synthetic benchmark application

## BUILT-IN AGENTS

**geopm_agent_monitor(7)**: Agent implementation that enforces no policies

**geopm_agent_energy_efficient(7)**: Agent for saving energy

**geopm_agent_power_balancer(7)**: Agent that optimizes performance under a power cap

**geopm_agent_power_governor(7)**: Agent that enforces a power cap

## RUNTIME CONTROL

**geopm_ctl_c(3)**: GEOPM runtime control thread

**geopmctl(1)**: GEOPM runtime control application

**geopm_agent_c(3)**: Query information about available agents

**geopm_endpoint_c(3)**: Dynamic policy control for resource management

**geopmendpoint(1)**: Command line tool for dynamic policy control

**geopmagent(1)**: Query agent information and create static policies

**geopm_policy_c(3)**: GEOPM runtime policy management [DEPRECATED]

**geopmpolicy(1)**: GEOPM configuration creator and static mode enforcer [DEPRECATED]

## MISC

**geopm_error(3)**: Error code descriptions

**geopm_version(3)**: GEOPM library version

**geopm_sched(3)**: Interface with Linux scheduler


## INTEGRATION WITH PMPI
Linking to libgeopm will define symbols that intercept the MPI
interface through PMPI.  This can be disabled with the configure time
option `--disable-pmpi`, but is enabled by default.  See
`LD_DYNAMIC_WEAK` environment variable description below for the
runtime requirements of the PMPI design.  When using the GEOPM PMPI
interposition other profilers which use the same method will be in
conflict.  The GEOPM runtime can create an application performance
profile report and a trace of the application runtime.  As such, GEOPM
serves the role of an application profiler in addition to management
of power resources.  The report and trace generation are controlled by
the environment variables `GEOPM_REPORT` and `GEOPM_TRACE`; see
description below.

## INTEGRATION WITH OMPT
When the OpenMP runtime compiled with GEOPM supports the OMPT
interface, GEOPM will use the OMPT callbacks to wrap OpenMP parallel
regions with calls to `geopm_prof_enter()` and `geopm_prof_exit()`.
In this way, any OpenMP parallel region not within another
application-defined region will be reported to the GEOPM runtime.
This will appear in the report as a region name beginning with
"[OMPT]" and referencing the object file and function name containing
the OpenMP parallel region e.g.

`[OMPT]tutorial_6:geopm::StreamModelRegion::run()`

To expressly enable this feature, pass the `--enable-ompt` configure
flag at GEOPM build time.  This will build and install the LLVM OpenMP
runtime configured to support OMPT if the default OpenMP runtime does
not support the OMPT callbacks.  Note that your compiler must be
compatible with the LLVM OpenMP ABI for extending it in this way.

## LAUNCHING THE RUNTIME
The recommended methods for launching the GEOPM runtime are the job
launch wrapper scripts **geopmsrun(1)** and **geopmaprun(1)**.  See
these man pages for details about the command line interface.  If your
system does not support `aprun` or `srun` for job launch, please make
a change request for support of the job launch method used on your
system at the github issues page:

https://github.com/geopm/geopm/issues

Also, consider porting your job launch command into the
geopmpy.launcher module and submitting a change request as described
in CONTRIBUTING.md.

If the job launch application is not supported by the geopmpy.launcher
the recommended method is to use the environment variables described
in this man page including the `GEOPM_PMPI_CTL` environment variable.
If using the "application" launch method then the **geopmctl(1)**
application should be launched in parallel.

There are legacy methods for launching the runtime programmatically.
These are documented in **geopm_ctl_c(3)**, but are deprecated as an
application-facing interface because their use within an application
is incompatible with the GEOPM launcher script.

## PLOTTING REPORT AND TRACE DATA
Please see the **geopmplotter(1)** and **geopmanalysis(1)** man pages
for information about the data analysis tools available for
visualizing the data contained in the GEOPM report and trace files.
The report file is intended to be human readable, and the trace file
format can be easily parsed by a wide variety of analysis
applications.

## INTERPRETING THE REPORT
If the `GEOPM_REPORT` environment variable is set then a report will
be generated.  There is one report file generated for each run.  This
file has a header that describes the GEOPM version and the policies
that were used during the run.  This is followed by a breakdown first
by compute node host name and then by each compute region.  The report
file gives information about runtime and energy use for each
identified code region. Additionally, time spent in calls to MPI is
reported.  In the future the report may contain more data describing
the application.

It is important to note that per-region accounting in the report
includes only time spent in the region by all MPI ranks concurrently
on each compute node.  During the time when two ranks on a compute
node are not in the same marked region, the data collected is
attributed to the "unmarked" code region.  See the
`GEOPM_REGION_BARRIER` environment variable for more information about
debugging issues related region synchronicity.  In the future the
scope of this requirement may change from all ranks on a node to all
ranks running within the same domain of control.

## INTERPRETING THE TRACE
If the `GEOPM_TRACE` environment variable is set (see below) then a
trace file with time ordered information about the application runtime
is generated.  A separate trace file is generated for each compute
node and each file is a pipe (the `|` character) delimited ASCII
table. The file begins with a header that is marked by lines that
start with the `#` character.  The header contains information about
the GEOPM version and the policy that was used during the run.

The first row following the header gives a description of each field.
A simple method for selecting fields from the trace file is with the
`awk` command:

    $ grep -v '^#' geopm.trace-host0 | awk -F\| '{print $1, $2, $11}'

will print a subset of the fields in the trace file called
"geopm.trace-host0".

## ENVIRONMENT
When using the launcher wrapper scripts **geopmsrun(1)** or
**geopmaprun(1)**, the interface to the GEOPM runtime is controlled
exclusively by the launcher command line options.  The launcher script
sets the environment variables described in this section according to
the options specified on the command line.  Direct use of these
environment variables is only recommended when launching the GEOPM
runtime without **geopmsrun(1)** or **geopmaprun(1)**.

  * `LD_DYNAMIC_WEAK`:
    When dynamically linking an application to libgeopm for any
    features supported by the PMPI profiling of the MPI runtime it may
    be required that the LD_DYNAMIC_WEAK environment variable be set
    at runtime as is documented in the **ld.so(8)** man page.  When
    dynamically linking an application, if care is taken to link the
    libgeopm library before linking the library providing the weak MPI
    symbols, e.g. "-lgeopm -lmpi", linking order precedence will
    enforce the required override of the MPI interface symbols and the
    LD_DYNAMIC_WEAK environment variable is not required at runtime.

  * `GEOPM_REPORT`:
    If set then a report will be generated by the controller when it
    is destroyed.  The value of the variable determines the name of
    file generated.  The report contains a summary of performance and
    power aggregated over the program execution time and split out by
    host compute node and each code region.

  * `GEOPM_TRACE`:
    Enables GEOPM tracing capability.  Setting this variable enables
    the creation of a trace output file. The value of the variable is
    used as the base of the output trace file path.  One trace file is
    generated per compute node.  The file names of each trace are
    constructed by appending the node's hostname to base name given by
    the environment variable (separated by a '-').

  * `GEOPM_TRACE_SIGNALS`:
    Used to insert additional columns into the trace beyond the
    default columns and the columns added by the Agent.  The value
    must be formatted as a comma-separated list of valid signal names.
    All custom signals added to the trace will be sampled and
    aggregated for the entire node.  For example, the following will
    add total DRAM energy and power as columns in the trace:

    `GEOPM_TRACE_SIGNALS=ENERGY_DRAM,POWER_DRAM`

    The signals available and their descriptions are listed below.
    "TIME", "REGION_ID#", "REGION_PROGRESS", "REGION_RUNTIME",
    "ENERGY_PACKAGE", "POWER_PACKAGE", and "FREQUENCY" are included
    in the trace by default.

    `TIME` - time elaspsed since the beginning of execution. <br>
    `REGION_ID#` - 64-bit integer region ID that all ranks are
                   running, including MPI flag (bit 62) if applicable.
                   A nested MPI region for the region ID
                   0x0000000012345678 will appear as
                   0x4000000012345678.  If all ranks are not in the
                   same region, it will be recorded as unmarked
                   (0x2000000000000000).  Epoch boundaries are
                   recorded as entry and exit to the epoch region
                   (0x8000000000000000). <br>
    `REGION_PROGRESS` - minimum per-rank reported progress through the
                        current region. <br>
    `REGION_RUNTIME` - maximum per-rank last recorded runtime for the
                       current region. <br>
    `ENERGY_PACKAGE` - total energy for all sockets on the node. <br>
    `POWER_PACKAGE` - total power for all sockets on the node. <br>
    `FREQUENCY` - average frequency of all cores. <br>
    `ENERGY_DRAM` - total energy for all DRAM on the node. <br>
    `POWER_DRAM` - total power for all DRAM on the node. <br>
    `POWER_PACKAGE_MIN` - minimum setting for package power. <br>
    `POWER_PACKAGE_MAX` - maximum setting for package power. <br>
    `CYCLES_THREAD` - average clock cycles executed by cores since the
                      beginning of execution. <br>
    `CYCLES_REFERENCE` - average clock reference cycles since the beginning of
                         execution. <br>

  * `GEOPM_AGENT`:
    Used to select the Agent to be used by all Kontrollers.  The Agent
    will take over the role previously held by Deciders in splitting
    policy information to be sent to controllers lower in the tree,
    and aggregating sampled data up the tree.  Available agents are:
    "monitor", "power_balancer", "power_governor", and
    "energy_efficient".

  * `GEOPM_POLICY`:
    Specifies a JSON file path containing the policy.  If the policy
    is provided through this file, it will only be read once and
    cannot be changed dynamically.  In this mode, samples will not be
    provided to the resource manager.  GEOPM_POLICY and GEOPM_ENDPOINT
    cannot be set simultaneously; refer to the documentation for
    GEOPM_ENDPOINT for more details.

  * `GEOPM_ENDPOINT`:
    Combined with GEOPM_SHMKEY, serves as the base name for policy and
    sample shared memory files used to communicate with the resource
    manager.  It should not contain the '/' character.  The location
    for policy values written by the resource manager and read by the
    Controller will be "/$(GEOPM_SHMKEY)-$(GEOPM_ENDPOINT).policy" and
    the location for sample values written by the Controller and read
    by the resource manager will be
    "/$(GEOPM_SHMKEY)-$(GEOPM_ENDPOINT).sample".  GEOPM_ENDPOINT and
    GEOPM_POLICY cannot both be set simultaneously; the choice of
    which to set is determined by whether the Controller should use
    shared memory to receive policies dynamically from the resource
    manager, or use a JSON file to read a fixed policy.  One or the
    other must be set when launching the GEOPM controller through the
    PMPI interface (see GEOPM_PMPI_CTL environment variable below).

  * `GEOPM_SHMKEY`:
    Override the default shared memory key base.  The shared memory
    key base prefixes all shared memory keys used by GEOPM to
    communicate between the compute application and the GEOPM runtime.
    The default key base is '/geopm-shm', this can be overridden by
    the environment variable.  A shared memory key must begin with the
    '/' character and have no other occurrence of the '/' character.
    The base key is extended for each shared memory region used by the
    runtime.  These keys should be cleaned up by the exception handler
    in case of an error or OS signal other than SIGKILL.  If the
    application is killed with SIGKILL or the keys are left behind for
    any other reason, a simple command to clean up after an aborted
    job is:

    `$ test -n "$GEOPM_SHMKEY" && rm -f /dev/shm${GEOPM_SHMKEY}* || rm -f /dev/shm/geopm-shm*`

  * `GEOPM_PMPI_CTL`:
    When set to 'process' or 'pthread' this environment variable
    enables the launch of the GEOPM controller through the PMPI
    wrappers: in particular the wrappers for MPI_Init() or
    MPI_Init_thread().  In the case of specifying 'process' the
    controller will run as separate processes on a split off
    communicator comprised of the lowest rank on each node.
    Additionally through interception with the PMPI wrappers, the
    application MPI_COMM_WORLD appears to have one fewer rank per node
    than was allocated to the job.  In the case of specifying
    'pthread' the lowest rank on each node will launch a pthread
    running the controller.  The pthread launch mechanism requires an
    MPI runtime that can support MPI_THREAD_MULTIPLE.  Support for
    this feature may require the LD_DYNAMIC_WEAK variable as
    documented above.

  * `GEOPM_PROFILE`:
    If set, will override the profile name to the value specified.
    The default profile name is the name of the compute application
    executable.  The profile name is printed in the report and trace
    and can be used to index the report and trace data during
    post-processing.  When launching the GEOPM runtime with the geopmctl
    application, this variable must be set in the environment for
    the compute application and the geopmctl application.  The value
    of the variable must be the same in both contexts.

  * `GEOPM_PROFILE_TIMEOUT`:
    If set, will enable changing of the GEOPM profiler timeout.  The
    value is in seconds and represents the amount of time any application
    rank will wait for the controller before continuing execution. The
    default timeout is 30 seconds.

  * `GEOPM_PLUGIN_PATH`:
    The search path for GEOPM plugins. It is a colon-separated list
    of directories used by GEOPM to search for shared objects which
    contain GEOPM plugins.  In order to be available to the GEOPM
    runtime, plugins should register themselves with the appropriate
    factory.  Refer to tutoral/iogroup and tutorial/agent for examples
    of plugins that register themselves at plugin load time. A
    zero-length directory name indicates the current working
    directory.  A zero-length directory is inferred by a leading or
    trailing colon or two adjacent colons.  The default search
    location is always searched and is determined at library
    configuration time and by way of the 'pkglib' variable (typically
    /usr/lib64/geopm/).

  * `GEOPM_DEBUG_ATTACH`:
    Enables a serial debugger such as gdb to attach to a job when the
    GEOPM PMPI wrappers are enabled.  If set to a numerical value the
    associated rank will be trapped in MPI_Init() until a debugger is
    attached and the local variable "cont" is set to a non-zero value.
    If set, but not to a numerical value then all ranks are trapped.
    The runtime will print a message explaining the hostname and
    process ID that the debugger should attach to.

  * `GEOPM_EXEC_WRAPPER`:
    Configure string for GEOPM launcher to run arbitrary
    application to wrap target job execution.

    `$ export GEOPM_EXEC_WRAPPER="numactl -m 1"`

    `$ export GEOPM_EXEC_WRAPPER="valgrind --tool=memcheck --vgdb=yes \
                                  --vgdb-error=0"`

    `$ export GEOPM_EXEC_WRAPPER="$PATH_TO_DDT_CLIENT_BIN --ddtsessionfile \
                                  $PATH_TO_DDT_CLIENT_SESSION_FILE"`

  * `GEOPM_REGION_BARRIER`:
    Enables a node local MPI_Barrier() at time of calling
    `geopm_region_enter`() or `geopm_region_exit`() for all
    application ranks that share a node.  Since the GEOPM controller
    only considers a region to be entered when all ranks on a node
    have entered the region, enabling this feature forces control
    throughout all of the time every rank spends in a region.  This
    feature is primarily used for debugging purposes.  WARNING: If all
    regions marked in the application are not entered synchronously by
    all ranks on a node then enabling this feature will cause a
    deadlock and the application will hang.

  * `GEOPM_RM`:
    Used by job launch wrapper (geopmsrun or geopmaprun) to override
    the resource manager to use for job launch.  This environment
    variable serves the same purpose as the `--geopm-rm` command line
    option.  The valid values for the variable or command line option
    are detailed in the **geopmsrun(1)** or **geopmaprun(1)** man
    pages.  Note that the command line option overrides what is set in
    the environment.  If not set the script will attempt to determine
    the resource manager based on the executable name, your PATH and
    hostname.  This variable is also used by the integration tests.

  * `GEOPM_COMM`:
    Used to select the plugin description string for communication.
    The communication plugin is not currently used, but in the future
    the the comm plugin will be used for all communication between
    Controllers running on different nodes.  The only installed
    communication plugin uses MPI and the description string is "MPIComm".
    The user may wish to implement other communication mechanisms.

  * `GEOPM_DISABLE_HYPERTHREADS`:
    Used by job launch wrapper (geopmsrun or geopmaprun) to disable
    the usage of hyperthreads for pinning purposes.  The default behavior
    is to use hyperthreads for pinning.  This environment variable serves
    the same purpose as the `--geopm-disable-hyperthreads` command line
    option.  If the environment variable is set and/or the command line
    option is specified then hyperthreads will be disabled.

## DEPRECATION NOTICE AND BETA INTERFACE CHANGES
As of the GEOPM version 0.5.0 beta release, a major refactor of the
controller and plugin architecture is provided as an optional new code
path.  The old code path is still available for users as long as the
controller is run without the `GEOPM_AGENT` environment variable set.
The new code path will be active if the user selects an agent by name
with the `GEOPM_AGENT` environment variable when launching the
controller.  The old code path is maintained in the current Controller
object along with the the Decider / Platform / PlatformImp plugins.
The new code path is maintained in a replacement for the Controller
which has been temporarily named the Kontroller.  The Kontroller will
be renamed the Controller after the version 0.5.0 release, and the old
code path will no longer be available.

The beta release enables a new set of plugin interfaces named the
IOGroup, Agent, and Comm.  It is through the IOGroup, Agent and Comm
plugins that the GEOPM runtime can be extended.  The Decider /
Platform / PlatformImp plugin extensions are deprecated and will be
removed after this release.  The IOGroup plugin enables a user to add
new signal and control mechanisms for an Agent to read and write.  The
Agent plugin enables a user to add new monitor and control algorithms
to the GEOPM runtime.  MPI use by the GEOPM runtime which is not
linked by application has been completely encapsulated in Comm object.
The tutorial has been extended with two new directories: tutorial/agent
and tutorial/iogroup.  The tutorial/iogroup directory documents how to
write an IOGroup plugin.  The tutorial/agent directory documents how
to write an Agent plugin.

The primary motivations for these changes are increasing the
flexibility of extensions to GEOPM and improving testablity.  One
significant problem solved by the IOGroup/PlatformIO replacement of
the Platform/PlatformImp design is the ability to support multiple
plugins simultaneously for extending telemetry and control.  The Agent
interface enables a much more fully featured modification of the
control algorithm.  Additionally the Agent definition enables the
GEOPM resource manager interface to provide Agent-specific
requirements.  In related changes to the inter-node communication, the
GEOPM runtime now supports arbitrary samples and policies to be
communicated between agents across the network.

For more information regarding these changes and their effects on
command line executables see: **geopmagent(1)** and **geopmendpoint(1)**.

For more information regarding these changes and their effects on C
interfaces: **geopm_agent_c(3)** and **geopm_endpoint_c(3)**

## COPYRIGHT
Copyright (c) 2015, 2016, 2017, 2018, Intel Corporation. All rights reserved.

## SEE ALSO
**geopmpy(7)**,
**geopm_agent_energy_efficient(7)**,
**geopm_agent_monitor(7)**,
**geopm_agent_power_balancer(7)**,
**geopm_agent_power_governor(7)**,
**geopm_agent_c(3)**,
**geopm_ctl_c(3)**,
**geopm_endpoint_c(3)**,
**geopm_error(3)**,
**geopm_fortran(3)**,
**geopm_policy_c(3)**,
**geopm_prof_c(3)**,
**geopm_sched(3)**,
**geopm_version(3)**,
**geopm-Agg(3)**,
**geopm-CircularBuffer(3)**,
**geopm-Comm(3)**,
**geopm-CpuinfoIOGroup(3)**,
**geopm-EnergyEfficientAgent(3)**,
**geopm-EnergyEfficientRegion(3)**,
**geopm-Exception(3)**,
**geopm-Helper(3)**,
**geopm-MPIComm(3)**,
**geopm-MSR(3)**,
**geopm-MSRIO(3)**,
**geopm-MSRIOGroup(3)**,
**geopm-MonitorAgent(3)**,
**geopm-PlatformTopo(3)**,
**geopm-PluginFactory(3)**,
**geopm-PowerBalancer(3)**,
**geopm-PowerBalancerAgent(3)**,
**geopm-PowerGovernor(3)**,
**geopm-PowerGovernorAgent(3)**,
**geopm-ProfileIO(3)**,
**geopm-ProfileIOGroup(3)**,
**geopm-ProfileIOSample(3)**,
**geopm-RegionAggregator(3)**,
**geopm-SharedMemory(3)**,
**geopm-TimeIOGroup(3)**,
**geopmagent(1)**,
**geopmanalysis(1)**,
**geopmaprun(1)**,
**geopmbench(1)**,
**geopmctl(1)**,
**geopmendpoint(1)**,
**geopmplotter(1)**,
**geopmpolicy(1)**,
**geopmread(1)**,
**geopmsrun(1)**,
**geopmwrite(1)**,
**ld.so(8)**
