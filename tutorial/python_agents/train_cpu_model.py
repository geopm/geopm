#!/usr/bin/env python3

#  Copyright (c) 2015 - 2021, Intel Corporation
#
#  Redistribution and use in source and binary forms, with or without
#  modification, are permitted provided that the following conditions
#  are met:
#
#      * Redistributions of source code must retain the above copyright
#        notice, this list of conditions and the following disclaimer.
#
#      * Redistributions in binary form must reproduce the above copyright
#        notice, this list of conditions and the following disclaimer in
#        the documentation and/or other materials provided with the
#        distribution.
#
#      * Neither the name of Intel Corporation nor the names of its
#        contributors may be used to endorse or promote products derived
#        from this software without specific prior written permission.
#
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
#  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
#  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
#  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
#  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
#  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
#  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
#  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
#  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY LOG OF THE USE
#  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
#

import tensorflow as tf
from tensorflow.keras.layers.experimental import preprocessing
import pandas as pd
import argparse

def main():
    NODES = 32

    parser = argparse.ArgumentParser(
        description='Run ML training based on CPU frequency sweep data.')
    parser.add_argument('input', help='HDF file containing the training data, '
                                      'generated by process_cpu_frequency_sweep.py.')
    parser.add_argument('output', help='Output directory for the tensorflow model.')
    parser.add_argument('--leave-app-out',
                        help='Leave the named app out of the training set')
    args = parser.parse_args()

    df_traces = pd.read_hdf(args.input)

    y_columns = ['phi-freq']
    X_columns = ['POWER_PACKAGE',
                 'POWER_DRAM',
                 'FREQUENCY',
                 'TEMPERATURE_CORE',
                 'MSR::UNCORE_PERF_STATUS:FREQ-package-0',
                 'MSR::UNCORE_PERF_STATUS:FREQ-package-1',
                 'QM_CTR_SCALED_RATE-package-0',
                 'QM_CTR_SCALED_RATE-package-1']

    ratios = [['ENERGY_DRAM', 'TIME'],
              ['INSTRUCTIONS_RETIRED-package-0', 'TIME'],
              ['INSTRUCTIONS_RETIRED-package-1', 'TIME'],
              ['ENERGY_PACKAGE-package-0', 'TIME'],
              ['ENERGY_PACKAGE-package-1', 'TIME'],
              ['MSR::APERF:ACNT-package-0', 'MSR::MPERF:MCNT-package-0'],
              ['MSR::APERF:ACNT-package-1', 'MSR::MPERF:MCNT-package-1'],
              ['MSR::PPERF:PCNT-package-0', 'MSR::MPERF:MCNT-package-0'],
              ['MSR::PPERF:PCNT-package-1', 'MSR::MPERF:MCNT-package-1'],
              ['MSR::PPERF:PCNT-package-0', 'MSR::APERF:ACNT-package-0'],
              ['MSR::PPERF:PCNT-package-1', 'MSR::APERF:ACNT-package-1']]

    for num,den in ratios:
        name = 'delta_{}/delta_{}'.format(num, den)
        X_columns.append(name)
        df_traces[name] = df_traces[num].diff() / df_traces[den].diff()
    X_columns.append("phi")

    # Exclude rows missing data in any of the columns of interest. Otherwise,
    # NaN values propagate into every weight in the model.
    is_missing_data = df_traces[X_columns + y_columns].isna().sum(axis=1) > 0
    df_traces = df_traces.loc[~is_missing_data]

    # Ignore applications that are requested to be ignored by the user. This
    # may be useful for a case where the training data includes many
    # application sweeps. Then, a single sweep output can be re-used for many
    # models, and each model can ignore one application. When evaluating the
    # model's performance on an applcation, we should use a model that excludes
    # that application from the training set so we can get a better idea about
    # how the model might generalize to unseen workloads.
    if args.leave_app_out is not None:
        if args.leave_app_out not in df_traces['app-config'].unique():
            print('Error: {args.leave_app_out} not in the available training sets')
            exit(1)
        df_traces = df_traces.loc[df_traces['app-config'] != args.leave_app_out]

    df_train = df_traces
    df_x_train = df_train[X_columns]
    df_y_train = df_train[y_columns]
    df_y_train /= 1e9

    #import pdb; pdb.set_trace()

    # The Normalization() function standardizes the input columns, so that
    # mean=0 and variance=1 for all columns of data. The linear transformation
    # to achieve those properties is saved as part of the model, and applied to
    # all incoming data automatically.
    normalize_layer = preprocessing.Normalization()
    normalize_layer.adapt(df_x_train)

    model = tf.keras.models.Sequential([
        normalize_layer,
        tf.keras.layers.Dense(len(X_columns), activation='sigmoid'),
        tf.keras.layers.Dense(len(X_columns), activation='sigmoid'),
        tf.keras.layers.Dense(len(X_columns), activation='sigmoid'),
        tf.keras.layers.Dense(len(y_columns))
    ])

    model.compile(optimizer='adam', loss='mean_absolute_error')

    model.fit(df_x_train, df_y_train, epochs=100, batch_size=500000)
    model.save(args.output)

if __name__ == "__main__":
    main()
