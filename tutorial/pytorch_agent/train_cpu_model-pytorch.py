#!/usr/bin/env python3
#  Copyright (c) 2015 - 2022, Intel Corporation
#  SPDX-License-Identifier: BSD-3-Clause
#

import torch
from torch import nn
import torch.utils.data as data
from random import uniform
import numpy as np
import sys
import os

import pandas as pd
import argparse
import code

from ray import tune
from ray.tune import CLIReporter
from ray.tune.schedulers import ASHAScheduler

from functools import partial

EPOCH_SIZE = 5
DEPTH_FC_MIN = 2
DEPTH_FC_MAX = 11

def main():
    parser = argparse.ArgumentParser(
        description='Run ML training based on CPU frequency sweep data.')
    parser.add_argument('input', help='HDF file containing the training data, '
                                      'generated by process_cpu_frequency_sweep.py.')
    parser.add_argument('output', help='Output directory for the tensorflow model.')
    parser.add_argument('--leave-app-out',
                        help='Leave the named app out of the training set')
    parser.add_argument('--train-hyperparams', action='store_true',
                        help='Train model hyper parameters')
    parser.add_argument('--train-hp-samples', type=int, default=20,
                        help='Numer of hyperparameter samples (tasks to execute).  '
                             'Only used when --train-hyperparams is specified.')
    args = parser.parse_args()

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print("Using {} device for training".format(device))

    df_traces = pd.read_hdf(args.input)

    y_columns = ['phi-freq']
    X_columns = ['CPU_POWER-package-0',
                 'CPU_FREQUENCY_STATUS-package-0',
                 'CPU_PACKAGE_TEMPERATURE-package-0',
                 'MSR::UNCORE_PERF_STATUS:FREQ-package-0',
                 'QM_CTR_SCALED_RATE-package-0']

    ratios = [['CPU_INSTRUCTIONS_RETIRED-package-0', 'CPU_CYCLES_THREAD-package-0'],
              ['CPU_INSTRUCTIONS_RETIRED-package-0', 'CPU_ENERGY-package-0'],
              ['MSR::APERF:ACNT-package-0', 'MSR::MPERF:MCNT-package-0'],
              ['MSR::PPERF:PCNT-package-0', 'MSR::MPERF:MCNT-package-0'],
              ['MSR::PPERF:PCNT-package-0', 'MSR::APERF:ACNT-package-0']]

    for num,den in ratios:
        name = 'delta_{}/delta_{}'.format(num, den)
        X_columns.append(name)
        df_traces[name] = df_traces[num].diff() / df_traces[den].diff()
    X_columns.append("phi")

    #Print phi to phi-freq mapping
    print(df_traces.pivot_table('phi-freq', 'phi', 'app-config'))

    # Exclude rows missing data in any of the columns of interest. Otherwise,
    # NaN values propagate into every weight in the model.
    # And replace Inf with NaN
    df_traces.replace([np.inf, -np.inf], np.nan, inplace=True)
    is_missing_data = df_traces[X_columns + y_columns].isna().sum(axis=1) > 0
    df_traces = df_traces.loc[~is_missing_data]

    # Assume no test traces to start
    df_test = None

    # Ignore applications that are requested to be ignored by the user. This
    # may be useful for a case where the training data includes many
    # application sweeps. Then, a single sweep output can be re-used for many
    # models, and each model can ignore one application. When evaluating the
    # model's performance on an applcation, we should use a model that excludes
    # that application from the training set so we can get a better idea about
    # how the model might generalize to unseen workloads.
    if args.leave_app_out is not None:
        app_config_list = [e for e in df_traces['app-config'].unique() if args.leave_app_out in e]
        if app_config_list is None:
            print('Error: {args.leave_app_out} not in the available training sets')
            exit(1)
        else:
            df_test_list = [df_test]
            for app_config in app_config_list:
                #If we exclude it from training we should save it for testing
                df_test_list.append(df_traces.loc[df_traces['app-config'] == app_config])

                #exclude from training
                df_traces = df_traces.loc[df_traces['app-config'] != app_config]

            #If using leave one out only use those cases for the test case
            df_test = pd.concat(df_test_list)

    train_size = int(0.8 * len(df_traces))
    val_size = len(df_traces) - train_size

    df_train = df_traces
    df_x_train = df_train[X_columns]
    df_y_train = df_train[y_columns]
    df_y_train /= 1e9

    x_train = torch.tensor(df_x_train.to_numpy()).float()
    y_train = torch.tensor(df_y_train.to_numpy()).float()
    # TODO: test this vs sending input and target
    #       this might be more performant overall, or easier
    x_train = x_train.to(device)
    y_train = y_train.to(device)

    train_set = torch.utils.data.TensorDataset(x_train, y_train)
    train_set, val_set = data.random_split(train_set, [train_size, val_size])

    batch_size = 1000

    if args.train_hyperparams:
        #TODO: evaluate sample mechanism used for each setting
        config = {
            "width_fc" : tune.sample_from(lambda _: 2**np.random.randint(2, 7)),
            "depth_fc" : tune.randint(2,11),
            "batch_size": tune.randint(500,3000),
            "lr" : tune.loguniform(1e-4, 1e-1)
        }

        scheduler = ASHAScheduler(
            metric="accuracy", #TODO: we may want to switch to using metric="loss", mode="min"
            mode="max",
            #max_t=100, #maximum time for a trial
            grace_period=1,
            reduction_factor=2)

        reporter = CLIReporter(
            metric_columns=["loss", "accuracy"])

        result = tune.run(
            tune.with_parameters(training_loop, input_size=len(X_columns), train_set=train_set, val_set=val_set),
            #resources_per_trial={"cpu":, "gpu":}, #TODO: specifying CPU availability
            config = config,
            num_samples=args.train_hp_samples,
            scheduler = scheduler,
            progress_reporter=reporter,
            checkpoint_at_end=True
            )

        best_trial = result.get_best_trial("accuracy", "max", "last")
        print("Best config: {}".format(best_trial.config))
        print("\taccuracy: {}".format(best_trial.last_result["accuracy"]))
        best_model = P3Net(width_input=len(X_columns), width_fc=best_trial.config["width_fc"], depth_fc=best_trial.config["depth_fc"])

        best_checkpoint_dir = best_trial.checkpoint.value
        model_state, optimizer_state = torch.load(os.path.join(
            best_checkpoint_dir, "checkpoint"))
        best_model.load_state_dict(model_state)

        batch_size = best_trial.config['batch_size']
    else:
        train_loader = torch.utils.data.DataLoader(dataset = train_set, batch_size = batch_size, shuffle = False)
        val_loader = torch.utils.data.DataLoader(dataset = val_set, batch_size = batch_size, shuffle = False)
        best_model = P3Net(width_input=len(X_columns), width_fc=len(X_columns), depth_fc=2)
        learning_rate = 1e-3
        optimizer = torch.optim.Adam(best_model.parameters(), lr=learning_rate)

        print("batch_size:{}, epoch_count:{}, learning_rate={}".format(batch_size, EPOCH_SIZE, learning_rate))
        for epoch in range(EPOCH_SIZE):
            print("\tepoch:{}".format(epoch))
            train(best_model, optimizer, train_loader, val_loader)

    print('Saving model (non-scripted and in training mode) as a precaution here: {}'.format(args.output + '-pre-torchscript'))
    torch.save(best_model.state_dict(), "{}".format(args.output + '-pre-torchscript'))

    best_model.eval()
    if df_test is not None:
        df_x_test = df_test[X_columns]
        df_y_test = df_test[y_columns]
        df_y_test /= 1e9

        x_test = torch.tensor(df_x_test.to_numpy()).float()
        y_test = torch.tensor(df_y_test.to_numpy()).float()
        x_test = x_test.to(device)
        y_test = y_test.to(device)

        test_set = torch.utils.data.TensorDataset(x_test, y_test)
        test_loader = torch.utils.data.DataLoader(dataset = test_set, batch_size = batch_size , shuffle = False)

        print("Begin Testing:")
        accuracy = test(best_model, test_loader);
        print('\tAccuracy vs test set: {:.2f}%.'.format(accuracy*100))

    model_scripted = torch.jit.script(best_model)
    model_scripted.save(args.output)
    print("Model saved to {}".format(args.output))

def training_loop(config, input_size, train_set, val_set):
    train_loader = torch.utils.data.DataLoader(dataset = train_set, batch_size = config['batch_size'], shuffle = False)
    val_loader = torch.utils.data.DataLoader(dataset = val_set, batch_size = config['batch_size'], shuffle = False)

    model = P3Net(width_input=input_size, width_fc=config["width_fc"], depth_fc=config["depth_fc"])
    learning_rate = config["lr"]
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    for epoch in range(EPOCH_SIZE):
        loss, accuracy = train(model, optimizer, train_loader, val_loader)
        # TODO: Initially the test set was used for accuracy determination, instead of reserving
        #       it for final testing.  This may provide better overall models, however it is likely
        #       better in the long term to rely on a train, validation, test set separation.
        #       This should be evaluated.  Simply not using --leave-app-out may help here.
        #test_accuracy = test(model, test_loader);
        #print('During training loop: Accuracy vs test set: {:.2f}%.'.format(test_accuracy*100))
        with tune.checkpoint_dir(epoch) as checkpoint_dir:
            path = os.path.join(checkpoint_dir, "checkpoint")
            torch.save((model.state_dict(), optimizer.state_dict()), path)

        # Send the current training result back to Tune
        tune.report(loss=loss, accuracy=accuracy)

    # Inference is going to be handled on the CPU
    # using a pytorch GEOPM agent.
    model.to(torch.device('cpu'))
    model.eval()

def train(model, optimizer, train_loader, val_loader):
    model.to(device)
    loss_fn = nn.MSELoss()

    for idx, (inputs, target_control) in enumerate(train_loader):
        model.train()
        # Clear gradient
        optimizer.zero_grad()
        # Get model output
        predicted_control = model(inputs)
        # loss calculation vs target
        loss = loss_fn(predicted_control, target_control)
        loss.backward()
        # update model weights
        optimizer.step()

    model.eval()
    prediction_total = 0
    prediction_correct = 0
    val_loss = 0
    val_steps = 0
    with torch.no_grad():
        for idx, (inputs, target_control) in enumerate(val_loader):
            prediction_total += inputs.size(0)

            # Run inputs through model, save prediction
            predicted_control = model(inputs)
            # Round to nearest 100 MHz increment
            predicted_control = np.round(predicted_control,1)
            prediction_correct += (target_control == predicted_control).sum().item()

            loss = loss_fn(predicted_control, target_control)
            val_loss += loss.numpy()
            val_steps += 1;


    return val_loss/val_steps, prediction_correct/prediction_total

def test(model, test_loader):
    model.to(device)

    prediction_total = 0
    prediction_correct = 0
    prediction_min = 0
    prediction_max = 0
    with torch.no_grad():
        for idx, (inputs, target_control) in enumerate(test_loader):
            prediction_total += inputs.size(0)
            # Run inputs through model, save prediction
            predicted_control = model(inputs)
            # Round to nearest 100 MHz increment
            predicted_control = np.round(predicted_control.cpu(),1)
            target_control = np.round(target_control.cpu(),1)

            # Generate stats
            prediction_correct += (target_control == predicted_control).sum().item()

            if (idx == 0):
                prediction_max = predicted_control.max()
                prediction_min = predicted_control.min()
            else:
                if (prediction_max < predicted_control.max()):
                    prediction_max = predicted_control.max()
                if (prediction_min > predicted_control.min()):
                    prediction_min = predicted_control.min()

        print('\ttest output min: {:.2f}, max: {:.2f}.'.format(prediction_min, prediction_max))
    return prediction_correct/prediction_total

class P3Net(nn.Module):
    def __init__(self, width_input, width_fc, depth_fc):
        super(P3Net, self).__init__()
        self.depth_fc = depth_fc
        self.norm1 = nn.BatchNorm1d(width_input)
        self.fc1 = nn.Linear(width_input, width_fc)
        # This approach doesn't work for torchscript - torch.jit.script(best_model)
        #self.fc_list = []
        #for d in range(self.depth_fc):
        #    self.fc_list.append(nn.Linear(width_fc, width_fc))
        self.fc2 = nn.Linear(width_fc, width_fc)
        self.sigmoid = nn.Sigmoid()
        self.fc_out = nn.Linear(width_fc, 1)

    def forward(self, x):
        x = self.norm1(x)
        x = self.fc1(x)
        x = self.sigmoid(x)
        # This approach doesn't work for torchscript - torch.jit.script(best_model)
        #for d in range(self.depth_fc):
        #    x = (self.fc_list[d])(x)
        #    x = self.sigmoid(x)
        for d in range(self.depth_fc):
            x = self.fc2(x)
            x = self.sigmoid(x)
        x = self.fc_out(x)
        return x

if __name__ == "__main__":
    main()
